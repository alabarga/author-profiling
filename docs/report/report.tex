% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%https://preview.overleaf.com/public/xnxjwzrdnynz/images/847ede20267048b902ed3c91cbcfe6aed1f31b8a.jpeg
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
  \pdfpagewidth=8.5truein
  \pdfpageheight=11truein
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\begin{document}

\title{Author Profiling from Personal Content Blogs}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{5} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor Aayush Singhal\\
       \affaddr{12CS10002}
% 2nd. author
\alignauthor Aseem Patni\\
       \affaddr{12CS10008}
% 3rd. author
\alignauthor Soham Dan\\
       \affaddr{12CS10059}\\
\and
% 4th. author
\alignauthor Bhushan Kulkarni\\
       \affaddr{12CS30016}
\alignauthor Pranay Yadav\\
       \affaddr{12CS30025}
% 6th. author
\alignauthor Shubham Saxena\\
       \affaddr{12CS30032}\\
\and 
\alignauthor Sruthi Warrier\\
       \affaddr{Mentor}
\alignauthor Anurag Verma\\
       \affaddr{Mentor}
\alignauthor Suman Kalyan Maity\\
       \affaddr{Mentor}\\
}
\date{30 July 2015}
\maketitle
\begin{abstract}
This project aims to predicting personally identifiable information (PII), such as age and gender of the author by extracting features from his/her personal content blog texts. We intend to define the state-of-the-art in the field and overcome the shortcomings of the prior works in the personality recognition tasks. This report describes our progress so far and contains details about our future work-flow.
\end{abstract}

\keywords{Authorship Profiling, PII, Blogosphere}

\section{Introduction}
Though the enormous impact of social media on our daily life, we observe a lack of
information about those who create the contents. In this regard, author profiling tries to
determine the gender, age, native language or personality type of authors by analyzing
their published texts. In this study, we focus on building a system to identify only the gender and age of the authors. Other authorship details will be a part of the future work in this area. Author profiling is of growing importance: E.g., from a marketing
viewpoint, companies may be interested in knowing the demographics of their target
group in order to achieve a better market segmentation; from a forensic viewpoint, determining
the linguistic profile of a person who wrote a "suspicious text" may provide
valuable background information.\\\\
This study is targeted towards partial fulfillment of requirements for \textit{CS60057: Speech \& Natural Language Processing} during Fall 2015, under the guidance of Prof. Pawan Goyal.\\\\
The remainder of this paper is organized as follows. Section 2 describes the corpus, Section 3 covers the proposed approach, Section 4 presents the results obtained so far, Section 5 discusses the evaluation measures, Section 6 contains details about the future work-plan and Section 7 concludes by listing the work done by the individual team-mates.\\
\section{Data-set}
\subsection{Corpus}
We have used the following two corpora for this study:
\begin{itemize}
\item Blog Authorship Corpus\\
The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words or approximately 35 posts and 7250 words per person. All bloggers included in the corpus fall into one of three age groups --- "10s" [13-17], "20s" [23-27], "30s" [33-47].
For each age group there are an equal number of male and female bloggers.   
\item PAN'14 Corpus\\
As a part of the Author Profiling Shared Task in PAN '14, this corpus was made available for use during the competition. This data-set originally consists of blog posts, tweets and social media texts written in both English and Spanish as well as hotel reviews in English. We have considered only the subset which contains blog posts. All bloggers included in the corpus fall into one of these age groups: [18-24], [25-34], [35-49], [50-64], [65-xx].
The corpus incorporates a total of 2278 posts, 148 authors or on an average 15 blogs per author. 
\end{itemize}
We split ourselves into two groups, one for extracting features from the Blog Authorship Corpus (Pranay, Shubham \& Soham) and the other from the PAN '14 Corpus (Aayush, Aseem \& Bhushan).  
\subsection{Data Cleaning \& Extraction}
\begin{itemize}
\item Blog Authorship Corpus\\
The corpus contains 19,320 XML files, each pertaining to a particular author, identified by the unique file-names. Each XML file contains date when the blog was posted followed by the post itself. All the HTML links in the post are replaced by a unique tag \textit{'urlLink'} to mark their presence. We cleaned the data by discarding empty blog posts and ignoring posts which contain only HTML links and no text. We then exported this refined data to a JSON file, on which further analysis will be carried out.
\item PAN '14 Corpus\\
This corpus contains 148 XML files, each pertaining to a particular author. Each XML file contains the Author's unique ID and blogs written by the Author. The blog text is present in \texttt{CDATA} section. To parse this text, we wrote a regular expression to remove the HTML tags, translated HTML entities like \texttt{'\&amp;', '\&ldquo;'} to their usual textual counter-parts like '\&','"'. We then dumped this data as to a JSON file, on which further analysis will be carried out.
\end{itemize}


\section{Approach}
After obtaining the JSON files containing refined data from the both the corpora, we now start extracting features from this data-set. We shall be focusing our attention towards building two kinds of classifiers --- 1) Binary classifier for classification of gender and 2) Multi-label classifier for classification of age into predefined class labels. Later, we will also consider the possibility of predicting the age by fitting regression models, by working under the assumptions that --- 1) There are enough data points for the model to fit accurately and 2) Age behaves like a continuous variable. 
\subsection{Exhaustive Feature Set} In this study, we shall consider the following types of features for building our classifiers:
\begin{itemize}
\item Content-based Features viz. \# of HTML links in the blog, \# of named entities used, \# of non-word errors, \# of discourse relations within the text, \# of quotations used in the text, \# of references to past or future within the text, \# of facts \& figures used, \# of times opinions are expressed, overall sentiment score of the blog, \# of words having character flooding (like 'hellooooo').
\item Style-based Features viz. distribution of POS tags, distribution of punctuation tags, readability measure of the blog (SMOG \& Fischer), \# of co-references (usage of pronouns), average sentence length, usage of figures of speech by the author (like metaphor, alliteration). 
\item Semantic Features: Latent Semantic Analysis (LSA) of the blogs to identify set of topics authors, belonging to a particular gender or age group, blog about.
\end{itemize}
These features will be extracted for both the corpora.
\subsection{Extracting Feature Vectors}
Methodology to extract feature vectors from the Blog Authorship Corpus is described below:
\begin{itemize}
\item Content-based Features:\\
1) \# of HTML links: count the occurrence of \textit{'urlLink'} in the body of the blog.\\
2) \# of named entities: Using Stanford NLTK APIs to tokenize blog text into sentences, perform POS Tagging and then extract named entities (NE) from the tagged sentences.\\
3) \# of non-word errors: Using Stanford NLTK's word corpus \texttt{nltk.corpus.words.words()} to keep a count of non-word errors in the blog.\\
4) \# of discourse relations within the text: Using a \href{https://github.com/ilija139/PDTB-Parser}{Java-based end-to-end PDTB-styled Discourse Parser} to identify implicit \& explicit discourse relations and keeping a count of each of them.\\
5) \# of quotations used in the text: Checking for occurrences of '"' in the blog text.\\
6) \# of references to past or future: Checking for occurrences of the following set of words and phrases - ['years ago','years from now','in the past','in future','once upon a time','\textasciicircum \textbackslash d\{4\}\$']. The last entry is a regex for detecting reference to a year.\\
7) \# of facts \& figures used: To be decided.\\
8) \# of times opinions are expressed: 
\end{itemize}
\subsection{Feature Subset Selection (FS)}
Dimensionality reduction (DR) and Feature Subset Selection (FS) are two techniques for reducing the attribute space of a feature set. The main idea of FS is to remove redundant or irrelevant features from the data set as they can lead to a reduction of the classification accuracy and to an unnecessary increase of computational cost. The advantage of FS is that no information about the importance of single features is lost. For now, we will focus our attention on using FS over DR because DR can decrease the size of the attribute space strikingly. Another important disadvantage of DR is the fact that the linear combinations of the original features are usually not interpretable and the information about how much an original attribute contributes is often lost. If possible, we might also try using DR technique (PCA) to our feature set and evaluate the improvement in the accuracy of the resulting classifier, if any.\\
There are three types of feature subset selection approaches:
\begin{itemize}
\item Filters:\\
Filters are classifier agnostic pre-selection methods which are independent of the later
applied machine learning algorithm. . Besides some statistical filtering methods like Fisher score or Pearson correlation, information gain is often used to find out
how well each single feature separates the given data set.\\
The overall entropy I of a given dataset S is defined as:
\begin{align}
I(S) := - \sum_{i=1}^{C} p_i log_2 p_i
\end{align}
where $C$ denotes the total number of classes and $p_i$ the portion of instances that belong to class $i$. The reduction in entropy or the information gain is computed for each attribute according to:
\begin{align}
IG(S,A) = I(S) - \sum_{v \in A} \frac{|S_{A,v}|}{|S|}I(S_{A,v})
\end{align}
where $v$ is a value of $A$ and $S_{A,v}$ is the set of instances where $A$ has value $v$.
\item Wrappers:\\
Wrappers are feedback methods which incorporate the ML algorithm in the FS process,
i.e. they rely on the performance of a specific classifier to evaluate the quality of a set of features. Wrapper methods search through the space of feature subsets and calculate
the estimated accuracy of a single learning algorithm for each feature that can be added
to or removed from the feature subset.
\end{itemize} 
We shall focus our attention towards using Filters to perform FS in this study.
\end{document}